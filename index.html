<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>PhotoDescribe — Live Object Detection</title>

<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">

<style>
html, body {
    margin: 0;
    padding: 0;
    width: 100%;
    height: 100%;
    background: #000;
    font-family: Arial, sans-serif;
}

#vanta-bg {
    position: fixed;
    width: 100%;
    height: 100%;
    z-index: 0;
}

.card-container {
    position: relative;
    z-index: 1;
    min-height: 100vh;
    display: flex;
    justify-content: center;
    align-items: center;
    padding: 20px;
}

.card {
    width: 100%;
    max-width: 640px;
    background: rgba(255,255,255,0.95);
    border-radius: 12px;
    padding: 16px;
}

#cameraWrapper {
    position: relative;
    width: 100%;
}

#cameraPreview {
    width: 100%;
    border-radius: 10px;
}

#overlay {
    position: absolute;
    top: 0;
    left: 0;
    pointer-events: none;
}

#instructionHint {
    position: absolute;
    top: 10px;
    left: 10px;
    background: rgba(0,0,0,.7);
    color: #fff;
    padding: 6px 10px;
    border-radius: 6px;
    font-size: 0.9em;
}

#resultBox {
    display: none;
}
</style>
</head>
<body>

<div id="vanta-bg"></div>

<div class="card-container">
<div class="card">

<h4 class="mb-2">PhotoDescribe — Live Object Detection</h4>
<p class="text-muted mb-3">
Camera opens automatically. Objects are detected live.
</p>

<div class="form-check mb-2">
    <input class="form-check-input" type="checkbox" id="ttsToggle" checked>
    <label class="form-check-label">Speak detected objects</label>
</div>

<div id="cameraWrapper">
    <video id="cameraPreview" autoplay playsinline></video>
    <canvas id="overlay"></canvas>
    <div id="instructionHint">Initializing AI…</div>
</div>

<div id="resultBox" class="mt-3 p-2 bg-light rounded">
    <strong>Detected:</strong>
    <span id="descriptionText"></span>
</div>

</div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r134/three.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/vanta@latest/dist/vanta.net.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

<script>
VANTA.NET({
    el: "#vanta-bg",
    color: 0x00ffff,
    backgroundColor: 0x000000,
    points: 14,
    maxDistance: 20,
    spacing: 15
});
</script>

<script>
class LiveDetectionApp {
    constructor() {
        this.video = document.getElementById("cameraPreview");
        this.overlay = document.getElementById("overlay");
        this.ctx = this.overlay.getContext("2d");
        this.hint = document.getElementById("instructionHint");
        this.text = document.getElementById("descriptionText");
        this.resultBox = document.getElementById("resultBox");
        this.ttsToggle = document.getElementById("ttsToggle");

        this.model = null;
        this.stream = null;
        this.lastSpoken = "";
    }

    async init() {
        this.hint.textContent = "Loading AI model…";
        this.model = await cocoSsd.load({ base: "lite_mobilenet_v2" });
        this.hint.textContent = "Opening camera…";
        await this.startCamera();
        this.hint.textContent = "Live detection running";
        this.detectLoop();
    }

    async startCamera() {
        this.stream = await navigator.mediaDevices.getUserMedia({ video: true });
        this.video.srcObject = this.stream;
        await new Promise(r => this.video.onloadedmetadata = r);

        this.overlay.width = this.video.videoWidth;
        this.overlay.height = this.video.videoHeight;
    }

    async detectLoop() {
        if (!this.stream) return;

        const predictions = await this.model.detect(this.video);

        this.draw(predictions);
        this.showResults(predictions);

        requestAnimationFrame(() => this.detectLoop());
    }

    draw(predictions) {
        this.ctx.clearRect(0, 0, this.overlay.width, this.overlay.height);
        this.ctx.strokeStyle = "#00E5FF";
        this.ctx.lineWidth = 3;
        this.ctx.font = "bold 14px Arial";

        predictions.forEach(p => {
            const [x, y, w, h] = p.bbox;
            this.ctx.strokeRect(x, y, w, h);

            const label = `${p.class} ${(p.score*100).toFixed(0)}%`;
            const tw = this.ctx.measureText(label).width;

            this.ctx.fillStyle = "#00E5FF";
            this.ctx.fillRect(x, y - 18, tw + 8, 18);
            this.ctx.fillStyle = "#000";
            this.ctx.fillText(label, x + 4, y - 4);
        });
    }

    showResults(predictions) {
        const names = [...new Set(predictions.map(p => p.class))];
        const text = names.length ? names.join(", ") : "No objects detected";

        this.text.textContent = text;
        this.resultBox.style.display = "block";

        if (this.ttsToggle.checked && text !== this.lastSpoken) {
            this.lastSpoken = text;
            this.speak(text);
        }
    }

    speak(text) {
        window.speechSynthesis.cancel();
        const u = new SpeechSynthesisUtterance(text);
        u.lang = "en-US";
        window.speechSynthesis.speak(u);
    }
}

document.addEventListener("DOMContentLoaded", () => {
    new LiveDetectionApp().init();
});
</script>

</body>
</html>
